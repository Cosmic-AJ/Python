{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d21cc19",
   "metadata": {},
   "source": [
    " Name: Ayush Jain \n",
    " \n",
    " Email: ayushjain051001@gmail.com\n",
    " \n",
    " Phone: 8560036606"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566698eb",
   "metadata": {},
   "source": [
    "# Data Harvesting or Scraping of Walmart Website\n",
    "\n",
    "You have to harvest brownfield data from a public source. Please write a python harvester in class abstraction and scalable manner (Object Oriented Programming). The output should be tabular, cleanly structured data (.csv or .json), preferably with data for 500-1000 samples. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53e2cf1",
   "metadata": {},
   "source": [
    "## Importing the Libraries\n",
    "Importing all the required libraries for the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68e6e121",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.support.ui import Select\n",
    "from dateutil.parser import parse\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118f22af",
   "metadata": {},
   "source": [
    "## Creating a Scrapper Class which will scrap the data from the pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71971e6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Scrap:\n",
    "    def __init__(self):\n",
    "        # Installing ChromeDriver if not available and will create an object for the same. \n",
    "        self.driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "\n",
    "    def save_csv(self, date, name, title, sno, desc, rating):\n",
    "        # All the lists are tranformed to a dictionary for saving the data in a csv file. It helps in understanding the data easily.\n",
    "        scraped_data = pd.DataFrame({'S.No':sno, 'Title':title, 'Name':name, 'Description':desc, 'Date':date, 'Rating':rating})\n",
    "        scraped_data.to_csv('result.csv', index=False)\n",
    "        \n",
    "    def extract(self, endDate):\n",
    "        # We declare lists which will store data of each review.\n",
    "        \n",
    "        date = []\n",
    "        name =[]\n",
    "        title=[]\n",
    "        desc=[]\n",
    "        rating=[]\n",
    "        sno=[]\n",
    "        num=1\n",
    "        \n",
    "        # We try to run in a loop which will go on extracting data until and unless it finds a date less then the specified one or their are no more reviews available\n",
    "        while True:\n",
    "            # We convert the page source in a proper manner for easy processing and understanding\n",
    "            \"\"\"try:\"\"\"\n",
    "            soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "\n",
    "            # We run this loop till we scrap all the reviews from the page. We go through all the components one by one.\n",
    "            for tags in soup.findAll('div', attrs={'class':'customer-review-body'}):\n",
    "                # We convert the date to datatime format from the string.\n",
    "                b = parse(tags.find('span', attrs={'class':'review-date-submissionTime'}).text)\n",
    "                # It is the break condition as the review being read is older then the specified date\n",
    "                if b<endDate:\n",
    "                    break\n",
    "\n",
    "                # We save the data extracted from all the elements in the array.\n",
    "                sno.append(num)\n",
    "                date.append(tags.find('span', attrs={'class':'review-date-submissionTime'}).text)\n",
    "                # Not available is added in the place where the user hasn't provide any information\n",
    "                if tags.find('span', attrs={'class':'review-footer-userNickname'}) is not None:\n",
    "                    name.append(str(tags.find('span', attrs={'class':'review-footer-userNickname'}).text).capitalize())\n",
    "                else:\n",
    "                    name.append('Not Available')\n",
    "                if tags.find('h3', attrs={'class':'review-title'}) is not None:\n",
    "                    title.append(tags.find('h3', attrs={'class':'review-title'}).text)\n",
    "                else:\n",
    "                    title.append('Not Available')\n",
    "                if tags.find('div', attrs={'class':'review-text'}) is not None:\n",
    "                    desc.append(tags.find('div', attrs={'class':'review-text'}).text)\n",
    "                else:\n",
    "                    desc.append('Not Available')\n",
    "                if tags.find('span', attrs={'class':'seo-avg-rating'}).text is not None:\n",
    "                    rating.append(tags.find('span', attrs={'class':'seo-avg-rating'}).text)\n",
    "                else:\n",
    "                    rating.append('Not Available')\n",
    "                num+=1\n",
    "\n",
    "            # Break condition for the whole scraping process and it will close the browser if the condition holds true.\n",
    "            if b<endDate:\n",
    "                self.driver.quit()\n",
    "                break\n",
    "\n",
    "            # We navigate to the next page to scrap more reviews using button.\n",
    "            self.driver.find_element_by_class_name('paginator-btn-next').click()\n",
    "            \"\"\"except:\n",
    "            print('All the reviews has been read')\n",
    "            break\"\"\"\n",
    "                \n",
    "        self.save_csv(date, name, title, sno, desc, rating)\n",
    "                \n",
    "        \n",
    "    def find(self, search_url):\n",
    "        # We open the browser and open the given url.\n",
    "        self.driver.get(search_url)\n",
    "\n",
    "        # We navigate to the bottom of the webpage and click on the see all reviews section to navigate to the other page\n",
    "        self.driver.find_element_by_link_text(\"See all reviews\").click()\n",
    "\n",
    "        # We sort the items in the order of newest item first by selecting th option from the drop down menu using XPath\n",
    "        sel = Select(self.driver.find_element_by_xpath('/html/body/div[1]/div/div/div/div[1]/div/div[5]/div/div[2]/div/div[2]/div/div[2]/select'))\n",
    "        sel.select_by_value('submission-desc')\n",
    "        self.driver.refresh()\n",
    "\n",
    "        \n",
    "\n",
    "        # We need to extract data till 1 December 2021 only. Therefore it will be the end date.\n",
    "        endDate = datetime(2020, 12, 1)\n",
    "\n",
    "        self.extract(endDate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1abc15",
   "metadata": {},
   "source": [
    "## Using the scraper to scrap reviews from Walmart Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "785f2729",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 92.0.4515\n",
      "Get LATEST driver version for 92.0.4515\n",
      "Driver [C:\\Users\\User\\.wdm\\drivers\\chromedriver\\win32\\92.0.4515.107\\chromedriver.exe] found in cache\n"
     ]
    }
   ],
   "source": [
    "# We provide the url of the website to scrap data\n",
    "search_url=\"https://www.walmart.com/ip/Clorox-Disinfecting-Wipes-225-Count-Value-Pack-Crisp-Lemon-and-Fresh-Scent-3-Pack-75-Count-Each/14898365\"\n",
    "\n",
    "scraper = Scrap()\n",
    "scraper.find(search_url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
